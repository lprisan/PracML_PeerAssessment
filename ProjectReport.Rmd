---
title: 'Practical Machine Learning: Course Project'
author: "Luis P."
date: "08/20/2015"
output: html_document
---

<!--<2000 words, <=5 figures-->

# Executive Summary

This report describes briefly the preprocessing, exploratory analysis, training and testing/validation of a machine learning algorithm, using different wearable sensors to predict (i.e., automatically detect) the manner in which a set of weight exercises is done (e.g., correctly, or incorrectly in different ways), performed by several individuals. After exploring the data and selecting the most interesting features, we trained several algorithms for such prediction, leading us to use XXXXXXXXXXXXXXXXX, which achieved an accuracy of XXXXXXX (Kappa=XXXXX) on in-sample testing, XXXXX (Kappa=XXXXX) on out-of-sample tests, and XXXXXX (Kappa=XXXXX) in out-of-subject testing (which might be a good prediction for the accuracy of the algorithm with new subjects).


# Analysis Questions

Given the dataset for weight exercises available from [http://groupware.les.inf.puc-rio.br/har], we want to answer the following questions: 

1. Can we predict how the exercise was performed (**classe variable**) in terms of the other fields (accelerometer data)?
2. We want to build a predictor that is **independent of the subject training** (and we want to know how good will that predictor be for new subjects)

# Error rate measurement

There are different measurements of error that can be used to compare the algorithms we will train. In general, we will use **Cohen's Kappa** as the main error/accuracy measurement variable (which is also the one used by the caret package, by default).

Using this variable, there are several kinds of error that we can measure:

* In-sample error: How many of our initial training samples in the algorithm able to predict accurately? (this is an optimistic estimation of the error)
* Out-of-sample error: How many samples that our algorithm has never seen (but still from the same set of subjects) can we predict accurately? (this is slightly less optimistic)
* Out-of-subject error: How many samples from a new subject (never seen in the training phase) can we predict accurately? This is the most realistic estimation of accuracy for the algorithm's accuracy independently of the subject training, and for new subjects we have never seen


# The dataset

## Loading and preprocessing of data (cleaning the training dataset)

Once we download the data files and load them in R, a basic exploration of the dataset show us that several of the variables have mostly missing values. Thus, we remove those features as they are unlikely to contribute to the classification module. We also remove other non-interesting variables, such as the `timestamp` or the `window`, and ensure that the `classe` we want to predict and the subjects are factor variables (code not shown for brevity).
```{r, message=FALSE, echo=F, warning=F}
require(caret)
require(kernlab)

# Download the training and final test sets

#download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "./data/dataTrain.csv", method="curl", extra = "-k")
rawtrain <- read.csv("./data/dataTrain.csv", stringsAsFactors = F)
#summary(rawtrain)

#download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "./data/dataTest.csv", method="curl", extra="-k")
rawtest <- read.csv("./data/dataTest.csv")
#summary(rawtest)

# We go over the test samples, and determine which fields will contain non-NA data, to simplify the training set too and make the models simpler
discard_fields <- numeric()
for(i in 1:length(rawtest)){
  if(sum(is.na(rawtest[,i]))==20) discard_fields <- c(discard_fields, i)
}
cleantest <- rawtest[,-discard_fields]
cleantrain <- rawtrain[,-discard_fields]
cleantrain$user_name <- factor(cleantrain$user_name)
cleantrain$classe <- factor(cleantrain$classe)

# We further clean the sets, to remove the "window"- and "timestamp"-like variables which will not be used for prediction either
cleantrain <- cleantrain[,!grepl("window",names(cleantrain))] 
cleantrain <- cleantrain[,!grepl("timestamp",names(cleantrain))] 
cleantrain <- cleantrain[,!grepl("X",names(cleantrain))] 
```

## Splitting the data
After this initial cleanup, we split the dataset in the following fashion (again, code not shown for brevity):

1. We take all the samples of one of the subjects, and set them aside to estimate our **out-of-subject error** at the end of the process
2. With the remaining samples, we do a 60/20/20 split, so that we have:
    a. 60% of the samples for training the algorithms. Within this subset we can do a first estimation (and comparison) of the **in-sample error**
    b. 20% of the samples to test the algorithms and compare them against each other
    c. 20% of the samples to do the final testing of the **out-of-sample error**

```{r, message=FALSE, echo=F, warning=F}
set.seed(12345)
# Keep one subject out, as a validation set to see how much out-of-subject error we would have
subjout <- unique(cleantrain$user_name)[1]
isoos <- cleantrain$user_name==subjout
oosvalSet <- cleantrain[isoos,] # To do out-of-subject validation
nooosval <- cleantrain[!isoos,]

# We split the data into training, test and validation sets (60/20/20
inTrain <- createDataPartition(y=nooosval$classe, p=0.6, list=F)
trainingSet <- nooosval[inTrain,]
testvalSet <- nooosval[-inTrain,]
inTest <- createDataPartition(y=testvalSet$classe, p=0.5, list=F) # from the remaining 40%, half (20%) goes to test set, half to the validation set
testSet <- testvalSet[inTest,]
valSet <- testvalSet[-inTest,]
```

Once we have this clean training set, we further reduce the dimensionality of the dataset by removing near-zero variance features and highly-correlated predictors. 
```{r, message=FALSE, echo=F, warning=F}
# Remove near-zero variance variables (actually, none of them)
nzv <- length(nearZeroVar(trainingSet))
if(nzv>0) trainingSet <- trainingSet[,-nearZeroVar(trainingSet)]
# Remove highly-correlated (>0.9) variables
trainingSet <- trainingSet[, -findCorrelation(cor(trainingSet[,-c(1,54)]))]
```
Hence, our clean training set has `r (length(trainingSet)-1)` predictors.

# Exploratory analysis
Once we have our clean training dataset, we explore its variables (mostly, through density plots and Shapiro-Wilk tests for normality). Only one variable's set of graphs is shown here, for brevity. We also plot the outcome variable (`classe`) against the different variables, to see if there are clear important predictors for that variable (not shown for brevity). We actually find no clear single one.
```{r, echo=F, warning=FALSE, message=FALSE}
# Density and normality tests
i <- 1
#for(i in 1:(length(trainingSet)-1) ){
    par(mfrow=c(1,2))
    plot(density(trainingSet[,i]))
#    shapiro.test(trainingSet[1:5000,i])
    qqnorm(trainingSet[,i])
    qqline(trainingSet[,i])
#}

# for(i in 0:9){
#   print(featurePlot(x=trainingSet[,(i*5)+(1:5)],y=trainingSet$classe,plot="pairs"))
# }
# featurePlot(x=trainingSet[,51:52],y=trainingSet$classe,plot="pairs")
```

# Training different prediction models

Now, we train different algorithms we know for predictors:

* Naive Bayes (NB)
```{r, cache=T, warning=F, echo=F, message=F}
set.seed(1234)
#library(doParallel)
#cl <- makeCluster(detectCores())
#registerDoParallel(cl)
# We try different multiclass classification models, 

# Naive Bayes, with and without centering-scaling the variables
set.seed(7)
nbFit <- train(classe~.,data=trainingSet,method="nb")
```
* Linear Discriminant Analysis (LDA)
```{r, cache=T, warning=F, echo=F, message=F}
# LDA
set.seed(7)
ldaFit <- train(classe~.,data=trainingSet,preProcess=c("center","scale"),method="lda")
```
* Simple Decision Tree 
```{r, cache=T, warning=F, echo=F, message=F}
# Decision Trees
set.seed(7)
treeFit <- train(classe~.,data=trainingSet,method="rpart")
```
* Bagged Decision Trees
```{r, cache=T, warning=F, echo=F, message=F}
# Bagged trees
set.seed(7)
bagFit <- train(classe~.,data=trainingSet,method="treebag")
```
* Random Forest (RF)
```{r, cache=T, warning=F, echo=F, message=F}
# Random Forests
set.seed(7)
rfFit <- train(classe~.,data=trainingSet,method="rf", prox=T)
```
* K-Nearest Neighbors (KNN)
```{r, cache=T, warning=F, echo=F, message=F}
# Nearest Neighbors
set.seed(7)
knnFit <- train(classe~.,data=trainingSet,method="knn")
```
* Gradient-Boosted Trees (GBM)
```{r, cache=T, warning=F, echo=F, message=F}
# boosted trees gbm
set.seed(7)
gbmFit <- train(classe~.,data=trainingSet,method="gbm", verbose=F)
```
* Support Vector Machine (SVM)
```{r, cache=T, warning=F, echo=F, message=F}
# Support vector machine
set.seed(7)
svmFit <- train(classe~.,data=trainingSet,method="svmLinear")
```


## Comparing models: In-sample error

After training all these models, we can check out the accuracies/Kappas they give on the training samples (which themselves were resampled to select the final model):
```{r, echo=F, message=F, warning=FALSE, cache=T}
models <- list(NB=nbFit, LDA=ldaFit, TREE=treeFit, BAG=bagFit, RF=rfFit, KNN=knnFit, GBM=gbmFit, SVM=svmFit)

results <- resamples(models)
# summarize the distributions
summary(results)
# boxplots of results
bwplot(results)
# dot plots of results
dotplot(results)
```

From this initial comparison, we see that Random Forest seems to perform the best (average Accuracy 0.99, average Kappa 0.98). But... is this model overfitting? See below for further evaluations

## Comparing models: Out-of-sample error

To get a clearer idea of how much we are overfitting, we use the data from a different subject we had set aside from the beginning, and predict the `classe` on those samples again.

```{r, echo=F, message=F, warning=FALSE, cache=T}
library(xtable)
ooserr <- data.frame(Model=character(), Accuracy=numeric(), Kappa=numeric())

bestKappa <- -1
bestModel <- NA
# Calculate the errors
for (i in 1:length(models)){
    model <- models[[i]]
    
    cm <- confusionMatrix(predict(model,newdata=valSet),valSet$classe)
    ooserr[i,"Kappa"] <- cm$overall["Kappa"]
    ooserr[i,"Accuracy"] <- cm$overall["Accuracy"]
    ooserr[i,"Model"] <- names(models)[i]
    
    #If this model is better than the previous ones, we keep it 
    if(cm$overall["Kappa"] > bestKappa){
        bestKappa <- cm$overall["Kappa"]
        bestModel <- model
    }
}

ooserr.table <- xtable(ooserr)
#digits(ooserr.table)[c(2,6)] <- 0
print(ooserr.table,floating=FALSE, type="html")

# We get the final accuracies for the best model
cmbest <- confusionMatrix(predict(bestModel,newdata=testSet),testSet$classe)
```

Using again the winning model (Random Forest) on the remaining (in-subject) test set, we get a value for accuracy of `r cmbest$overall["Accuracy"]` (Kappa `r cmbest$overall["Kappa"]`).

## Comparing models: Out-of-subject error

Finally, to have an estimation of whether we are overfitting with this model to the training set subjects, we try the models on a totally different subject, unseen by the algorithms so far, obtaining the following accuracies:

```{r, echo=F, message=F, warning=FALSE, cache=T}
library(xtable)
oosuberr <- data.frame(Model=character(), Accuracy=numeric(), Kappa=numeric())

bestKappa <- -1
bestModel <- NA
# Calculate the errors
for (i in 1:length(models)){
    model <- models[[i]]
    
    cm <- confusionMatrix(predict(model,newdata=oosvalSet),oosvalSet$classe)
    oosuberr[i,"Kappa"] <- cm$overall["Kappa"]
    oosuberr[i,"Accuracy"] <- cm$overall["Accuracy"]
    oosuberr[i,"Model"] <- names(models)[i]
    
    #If this model is better than the previous ones, we keep it 
    if(cm$overall["Kappa"] > bestKappa){
        bestKappa <- cm$overall["Kappa"]
        bestModel <- model
    }
}

oosuberr.table <- xtable(oosuberr)
#digits(ooserr.table)[c(2,6)] <- 0
print(oosuberr.table,floating=FALSE, type="html")

# We get the final accuracies for the best model
cmbest2 <- confusionMatrix(predict(bestModel,newdata=oosvalSet),oosvalSet$classe)
```

Thus, we see how the winning subject-independent model is now Naive Bayes (accuracy of `r cmbest2$overall["Accuracy"]`, Kappa `r cmbest2$overall["Kappa"]`), and that Random Forests were overfitting on the subjects we were training with.

# Final Testing the model (results for the assignment submission)

Once we have taken this decision, we train our algorithm with the full training dataset provided (to squeeze out the maximum of prediction power from it), and we make the final test predictions to be uploaded for automated testing in Coursera (not executed here).

```{r, eval=FALSE}
# Re-train the winning algorithm with all the training data we have
set.seed(7)
finalFit <- train(classe~.,data=cleantrain,method="knn", verbose=F)

# We predict the final test set and write it to files for submission
answers <- predict(finalfit, newdata=cleantest)

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(answers)
```
